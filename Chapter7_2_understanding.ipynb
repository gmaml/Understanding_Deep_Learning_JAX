{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOxgwdKmiLytjYNv/u1H/KO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"zoT1ImihVeJd","executionInfo":{"status":"ok","timestamp":1695929191004,"user_tz":240,"elapsed":935,"user":{"displayName":"Gang Ma","userId":"15410091551689130595"}}},"outputs":[],"source":["import jax.numpy as jnp\n","import matplotlib.pyplot as plt\n","\n","import jax.random as jr\n","\n","from jax.typing import ArrayLike\n"]},{"cell_type":"code","source":["# Number of layers\n","K = 5\n","# Number of neurons per layer\n","D = 6\n","# Input layer\n","D_i = 1\n","# Output layer\n","D_o = 1\n","\n","# Make empty lists\n","all_weights = [None] * (K+1)\n","all_biases = [None] * (K+1)\n","\n","\n","rng = jr.PRNGKey(seed=0)\n","bias_key,weight_key = jr.split(rng, num=2)\n","\n","sub_bias_keys = jr.split(bias_key,num=K+1)\n","sub_weight_keys = jr.split(weight_key,num=K+1)\n","\n","\n","def init_params(keys,parameter_list:list, D, D_i, D_o,K, is_bias:bool=True)->list:\n","  for i,sub_key in enumerate(keys):\n","    if i==0:\n","      if is_bias:\n","        parameter_list[i] = jr.normal(sub_key,shape=(D, 1))\n","      else:\n","        parameter_list[i] = jr.normal(sub_key,shape=(D, D_i))\n","    elif i==K:\n","      if is_bias:\n","        parameter_list[i] = jr.normal(sub_key,shape=(D_o,1))\n","      else:\n","        parameter_list[i] = jr.normal(sub_key,shape=(D_o,D))\n","    else:\n","      if is_bias:\n","        parameter_list[i] = jr.normal(sub_key,shape=(D,1))\n","      else:\n","        parameter_list[i] = jr.normal(sub_key,shape=(D,D))\n","  return parameter_list\n","\n","all_weights = init_params(sub_weight_keys,all_weights, D, D_i, D_o,K, is_bias=False)\n","all_biases = init_params(sub_bias_keys,all_biases, D, D_i, D_o,K, is_bias=True)\n"],"metadata":{"id":"vhgr9dHTWXL6","executionInfo":{"status":"ok","timestamp":1695929197276,"user_tz":240,"elapsed":5878,"user":{"displayName":"Gang Ma","userId":"15410091551689130595"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Define the Rectified Linear Unit (ReLU) function\n","def ReLU(preactivation):\n","  activation = jnp.clip(preactivation,0.0)\n","  return activation\n",""],"metadata":{"id":"Gb_Y8ewcg22u","executionInfo":{"status":"ok","timestamp":1695929197277,"user_tz":240,"elapsed":25,"user":{"displayName":"Gang Ma","userId":"15410091551689130595"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def compute_network_output(net_input:ArrayLike, all_weights:list[ArrayLike], all_biases:list[ArrayLike]):\n","\n","  # Retrieve number of layers\n","  K = len(all_weights) -1\n","\n","  # We'll store the pre-activations at each layer in a list \"all_f\"\n","  # and the activations in a second list[all_h].\n","  all_f = [None] * (K+1)\n","  all_h = [None] * (K+1)\n","\n","  #For convenience, we'll set\n","  # all_h[0] to be the input, and all_f[K] will be the output\n","  all_h[0] = net_input\n","\n","  # Run through the layers, calculating all_f[0...K-1] and all_h[1...K]\n","  for layer in range(K):\n","      # Update preactivations and activations at this layer according to eqn 7.16\n","      # Remmember to use np.matmul for matrrix multiplications\n","      # TODO -- Replace the lines below\n","      all_f[layer] = all_biases[layer]+ jnp.matmul(all_weights[layer],all_h[layer])\n","      all_h[layer+1] = ReLU(all_f[layer])\n","\n","  # Compute the output from the last hidden layer\n","  # TO DO -- Replace the line below\n","  all_f[K] = all_biases[K]+ jnp.matmul(all_weights[K],all_h[K])\n","\n","  # Retrieve the output\n","  net_output = all_f[K]\n","\n","  return net_output, all_f, all_h"],"metadata":{"id":"fYfi6h-GhlG2","executionInfo":{"status":"ok","timestamp":1695929197278,"user_tz":240,"elapsed":22,"user":{"displayName":"Gang Ma","userId":"15410091551689130595"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Define in input\n","net_input = jnp.ones((D_i,1)) * 1.2\n","# Compute network output\n","net_output, all_f, all_h = compute_network_output(net_input,all_weights, all_biases)\n","print(\"True output = %3.3f, Your answer = %3.3f\"%(1.907, net_output[0,0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dcCGccMJkhjf","executionInfo":{"status":"ok","timestamp":1695929197279,"user_tz":240,"elapsed":19,"user":{"displayName":"Gang Ma","userId":"15410091551689130595"}},"outputId":"a3dfe328-6d16-44f2-c29f-10a423a45358"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["True output = 1.907, Your answer = 21.464\n"]}]},{"cell_type":"code","source":["def least_squares_loss(net_output, y):\n","  return jnp.sum((net_output-y) * (net_output-y))\n","\n","def d_loss_d_output(net_output, y):\n","    return 2*(net_output -y);\n"],"metadata":{"id":"lKW75gZ1974C","executionInfo":{"status":"ok","timestamp":1695929197280,"user_tz":240,"elapsed":14,"user":{"displayName":"Gang Ma","userId":"15410091551689130595"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["y = jnp.ones((D_o,1)) * 20.0\n","loss = least_squares_loss(net_output, y)\n","print(\"y = %3.3f Loss = %3.3f\"%(y, loss))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CJ1ioqMp-NdW","executionInfo":{"status":"ok","timestamp":1695929197772,"user_tz":240,"elapsed":504,"user":{"displayName":"Gang Ma","userId":"15410091551689130595"}},"outputId":"07039141-6ebd-490f-d666-bedeca2227e9"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["y = 20.000 Loss = 2.144\n"]}]},{"cell_type":"code","source":["# We'll need the indicator function\n","def indicator_function(x:ArrayLike):\n","  x_in = jnp.where(x>=0,1,0)\n","  return x_in\n"],"metadata":{"id":"XOr8pSzw-QTJ","executionInfo":{"status":"ok","timestamp":1695931346035,"user_tz":240,"elapsed":1313,"user":{"displayName":"Gang Ma","userId":"15410091551689130595"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Main backward pass routine\n","def backward_pass(all_weights, all_biases, all_f, all_h, y):\n","  K = len(all_weights)-1\n","\n","  # We'll store the derivatives dl_dweights and dl_dbiases in lists as well\n","  all_dl_dweights = [None] * (K+1)\n","  all_dl_dbiases = [None] * (K+1)\n","  # And we'll store the derivatives of the loss with respect to the activation and preactivations in lists\n","  all_dl_df = [None] * (K+1)\n","  all_dl_dh = [None] * (K+1)\n","  # Again for convenience we'll stick with the convention that all_h[0] is the net input and all_f[k] in the net output\n","\n","  # Compute derivatives of net output with respect to loss\n","  all_dl_df[K] = jnp.array(d_loss_d_output(all_f[K],y))\n","\n","  # Now work backwards through the network\n","  for layer in range(K,-1,-1):\n","    # TODO Calculate the derivatives of biases at layer this from all_dl_df[layer]. (eq 7.21)\n","    # NOTE!  To take a copy of matrix X, use Z=np.array(X)\n","    # REPLACE THIS LINE\n","    all_dl_dbiases[layer] = all_dl_df[layer]\n","\n","    # TODO Calculate the derivatives of weight at layer from all_dl_df[K] and all_h[K] (eq 7.22)\n","    # Don't forget to use np.matmul\n","    # REPLACE THIS LINE\n","    all_dl_dweights[layer] = all_h[layer]*all_dl_df[layer]\n","\n","    # TODO: calculate the derivatives of activations from weight and derivatives of next preactivations (eq 7.20)\n","    # REPLACE THIS LINE\n","    all_dl_dh[layer] = jnp.transpose(all_weights[K])\n","\n","\n","    if layer > 0:\n","      # TODO Calculate the derivatives of the pre-activation f with respect to activation h (deriv of ReLu function)\n","      # REPLACE THIS LINE\n","      all_dl_df[layer-1] = indicator_function(all_f[layer-1])* jnp.matmul(all_dl_dh[layer], all_dl_df[layer])\n","\n","  return all_dl_dweights, all_dl_dbiases"],"metadata":{"id":"Ij1W24RNH1sR","executionInfo":{"status":"ok","timestamp":1695931356454,"user_tz":240,"elapsed":410,"user":{"displayName":"Gang Ma","userId":"15410091551689130595"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["all_dl_dweights, all_dl_dbiases = backward_pass(all_weights, all_biases, all_f, all_h, y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":392},"id":"mZ8BxOWNYtIV","executionInfo":{"status":"error","timestamp":1695931536564,"user_tz":240,"elapsed":1750,"user":{"displayName":"Gang Ma","userId":"15410091551689130595"}},"outputId":"f337e377-256d-4167-fb64-c8dc89ebfee8"},"execution_count":12,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-840f7debaa09>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_dl_dweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_dl_dbiases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_biases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-11-dfafee037eaa>\u001b[0m in \u001b[0;36mbackward_pass\u001b[0;34m(all_weights, all_biases, all_f, all_h, y)\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0;31m# TODO Calculate the derivatives of the pre-activation f with respect to activation h (deriv of ReLu function)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0;31m# REPLACE THIS LINE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m       \u001b[0mall_dl_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindicator_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_f\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_dl_dh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_dl_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mall_dl_dweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_dl_dbiases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, precision)\u001b[0m\n\u001b[1;32m   3132\u001b[0m   \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_squeeze\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_squeeze\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3134\u001b[0;31m   out = lax.dot_general(\n\u001b[0m\u001b[1;32m   3135\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb_is_mat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3136\u001b[0m     precision=precision)\n","    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_dot_general_shape_rule\u001b[0;34m(lhs, rhs, dimension_numbers, precision, preferred_element_type)\u001b[0m\n\u001b[1;32m   2506\u001b[0m     msg = (\"dot_general requires contracting dimensions to have the same \"\n\u001b[1;32m   2507\u001b[0m            \"shape, got {} and {}.\")\n\u001b[0;32m-> 2508\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlhs_contracting_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs_contracting_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2510\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0m_dot_general_shape_computation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension_numbers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: dot_general requires contracting dimensions to have the same shape, got (1,) and (6,)."]}]},{"cell_type":"code","source":["np.set_printoptions(precision=3)\n","# Make space for derivatives computed by finite differences\n","all_dl_dweights_fd = [None] * (K+1)\n","all_dl_dbiases_fd = [None] * (K+1)\n","\n","# Let's test if we have the derivatives right using finite differences\n","delta_fd = 0.000001\n","\n","# Test the dervatives of the bias vectors\n","for layer in range(K):\n","  dl_dbias  = np.zeros_like(all_dl_dbiases[layer])\n","  # For every element in the bias\n","  for row in range(all_biases[layer].shape[0]):\n","    # Take copy of biases  We'll change one element each time\n","    all_biases_copy = [np.array(x) for x in all_biases]\n","    all_biases_copy[layer][row] += delta_fd\n","    network_output_1, *_ = compute_network_output(net_input, all_weights, all_biases_copy)\n","    network_output_2, *_ = compute_network_output(net_input, all_weights, all_biases)\n","    dl_dbias[row] = (least_squares_loss(network_output_1, y) - least_squares_loss(network_output_2,y))/delta_fd\n","  all_dl_dbiases_fd[layer] = np.array(dl_dbias)\n","  print(\"Bias %d, derivatives from backprop:\"%(layer))\n","  print(all_dl_dbiases[layer])\n","  print(\"Bias %d, derivatives from finite differences\"%(layer))\n","  print(all_dl_dbiases_fd[layer])\n","\n","\n","# Test the derivatives of the weights matrices\n","for layer in range(K):\n","  dl_dweight  = np.zeros_like(all_dl_dweights[layer])\n","  # For every element in the bias\n","  for row in range(all_weights[layer].shape[0]):\n","    for col in range(all_weights[layer].shape[1]):\n","      # Take copy of biases  We'll change one element each time\n","      all_weights_copy = [np.array(x) for x in all_weights]\n","      all_weights_copy[layer][row][col] += delta_fd\n","      network_output_1, *_ = compute_network_output(net_input, all_weights_copy, all_biases)\n","      network_output_2, *_ = compute_network_output(net_input, all_weights, all_biases)\n","      dl_dweight[row][col] = (least_squares_loss(network_output_1, y) - least_squares_loss(network_output_2,y))/delta_fd\n","  all_dl_dweights_fd[layer] = np.array(dl_dweight)\n","  print(\"Weight %d, derivatives from backprop:\"%(layer))\n","  print(all_dl_dweights[layer])\n","  print(\"Weight %d, derivatives from finite differences\"%(layer))\n","  print(all_dl_dweights_fd[layer])\n",""],"metadata":{"id":"fr_9NZr8ZYwl"},"execution_count":null,"outputs":[]}]}